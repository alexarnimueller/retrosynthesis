% Encoding: UTF-8

@Article{drugan,
  author  = {Kadurin, Artur and Nikolenko, Sergey and Khrabrov, Kuzma and Aliper, Alex and Zhavoronkov, Alex},
  title   = {druGAN: An Advanced Generative Adversarial Autoencoder Model for de Novo Generation of New Molecules with Desired Molecular Properties in Silico},
  journal = {Molecular Pharmaceutics},
  year    = {2017},
  volume  = {14},
  number  = {9},
  pages   = {3098-3104},
  note    = {PMID: 28703000},
  doi     = {10.1021/acs.molpharmaceut.7b00346},
  eprint  = {http://dx.doi.org/10.1021/acs.molpharmaceut.7b00346},
}

@Article{RNN,
  author      = {Marwin H. S. Segler and Thierry Kogej and Christian Tyrchan and Mark P. Waller},
  title       = {Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks},
  abstract    = {In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.},
  date        = {2017-01-05},
  eprint      = {1701.01329v1},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1701.01329v1:PDF},
  keywords    = {cs.NE, cs.AI, cs.LG, physics.chem-ph, stat.ML},
  review      = {This is really very stunning article I have ever read.},
}

@Article{ThomasGAN,
  author   = {Blaschke, Thomas and Olivecrona, Marcus and Engkvist, Ola and Bajorath, J\"{u}rgen and Chen, Hongming},
  title    = {Application of Generative Autoencoder in de Novo Molecular Design},
  journal  = {Molecular Informatics},
  pages    = {n/a--n/a},
  issn     = {1868-1751},
  doi      = {10.1002/minf.201700123},
  keywords = {Autoencoder, chemoinformatics, de novo molecular design, deep learning, inverse QSAR},
}

@Article{ErtlLSTM,
  author      = {Peter Ertl and Richard Lewis and Eric Martin and Valery Polyakov},
  title       = {In silico generation of novel, drug-like chemical matter using the LSTM neural network},
  journal     = {arXiv},
  year        = {2017},
  abstract    = {The exploration of novel chemical spaces is one of the most important tasks of cheminformatics when supporting the drug discovery process. Properly designed and trained deep neural networks can provide a viable alternative to brute-force de novo approaches or various other machine-learning techniques for generating novel drug-like molecules. In this article we present a method to generate molecules using a long short-term memory (LSTM) neural network and provide an analysis of the results, including a virtual screening test. Using the network one million drug-like molecules were generated in 2 hours. The molecules are novel, diverse (contain numerous novel chemotypes), have good physicochemical properties and have good synthetic accessibility, even though these qualities were not specific constraints. Although novel, their structural features and functional groups remain closely within the drug-like space defined by the bioactive molecules from ChEMBL. Virtual screening using the profile QSAR approach confirms that the potential of these novel molecules to show bioactivity is comparable to the ChEMBL set from which they were derived. The molecule generator written in Python used in this study is available on request.},
  date        = {2017-12-20},
  eprint      = {1712.07449v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1712.07449v2:PDF},
  keywords    = {cs.LG, q-bio.QM},
}

@Article{DuvenaudVAE,
  author  = {Rafael G\'{o}mez-Bombarelli and Jennifer N. Wei and David Duvenaud and Jos\'{e} Miguel Hern\'{a}ndez-Lobato and Benjamin S\'{a}nchez-Lengeling and Dennis Sheberla and Jorge Aguilera-Iparraguirre and Timothy D. Hirzel and Ryan P. Adams and Al\'{a}n Aspuru-Guzik},
  title   = {Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules},
  journal = {ACS Central Science},
  year    = {2018},
  volume  = {4},
  number  = {2},
  pages   = {268-276},
  doi     = {10.1021/acscentsci.7b00572},
}

@Article{,
  author      = {Matt J. Kusner and Brooks Paige and José Miguel Hernández-Lobato},
  title       = {Grammar Variational Autoencoder},
  abstract    = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
  date        = {2017-03-06},
  eprint      = {1703.01925v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1703.01925v1:PDF},
  keywords    = {stat.ML},
}

@Article{doi:10.1080/14686996.2017.1401424,
  author    = {Xiufeng Yang and Jinzhe Zhang and Kazuki Yoshizoe and Kei Terayama and Koji Tsuda},
  title     = {ChemTS: an efficient python library for de novo molecular generation},
  journal   = {Science and Technology of Advanced Materials},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {972-976},
  doi       = {10.1080/14686996.2017.1401424},
  eprint    = {https://doi.org/10.1080/14686996.2017.1401424},
  publisher = {Taylor \& Francis},
}

@Article{OlivecronaReinforcment,
  author  = {Marcus Olivecrona and Thomas Blaschke and Ola Engkvist hongming Chen},
  title   = {Molecular de-novo design through deep reinforcement learning},
  journal = {J Cheminform.},
  year    = {2017},
  volume  = {9},
  number  = {48},
  pages   = {1758-2946},
  doi     = {10.1186/s13321-017-0235-x},
}

@Article{Gupta,
  author  = {Anvita Gupta and Alex T. M\'{u}ller and Berend J.H. Huisma and Jens A. Fuchs and Petra Schneider and Gisbert Schneider},
  title   = {Generative Recurrent Networks for De Novo Drug Design},
  journal = {Mol. Inf.},
  year    = {2018},
  volume  = {37},
  number  = {1-2},
  pages   = {1700111},
}

@Article{Xu2017,
  author  = {Xu, Yuting and Ma, Junshui and Liaw, Andy and Sheridan, Robert P. and Svetnik, Vladimir},
  title   = {Demystifying Multitask Deep Neural Networks for Quantitative Structure–Activity Relationships},
  journal = {Journal of Chemical Information and Modeling},
  year    = {2017},
  volume  = {57},
  number  = {10},
  pages   = {2490-2504},
  note    = {PMID: 28872869},
  doi     = {10.1021/acs.jcim.7b00087},
  eprint  = {https://doi.org/10.1021/acs.jcim.7b00087},
  url     = { 
        https://doi.org/10.1021/acs.jcim.7b00087
    
},
}

@Article{,
  author      = {Esben Jannik Bjerrum},
  title       = {SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules},
  abstract    = {Simplified Molecular Input Line Entry System (SMILES) is a single line text representation of a unique molecule. One molecule can however have multiple SMILES strings, which is a reason that canonical SMILES have been defined, which ensures a one to one correspondence between SMILES string and molecule. Here the fact that multiple SMILES represent the same molecule is explored as a technique for data augmentation of a molecular QSAR dataset modeled by a long short term memory (LSTM) cell based neural network. The augmented dataset was 130 times bigger than the original. The network trained with the augmented dataset shows better performance on a test set when compared to a model built with only one canonical SMILES string per molecule. The correlation coefficient R2 on the test set was improved from 0.56 to 0.66 when using SMILES enumeration, and the root mean square error (RMS) likewise fell from 0.62 to 0.55. The technique also works in the prediction phase. By taking the average per molecule of the predictions for the enumerated SMILES a further improvement to a correlation coefficient of 0.68 and a RMS of 0.52 was found.},
  date        = {2017-03-21},
  eprint      = {1703.07076v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1703.07076v2:PDF},
  keywords    = {cs.LG},
}

@Article{KAGGLE,
  author  = {Ma, Junshui and Sheridan, Robert P. and Liaw, Andy and Dahl, George E. and Svetnik, Vladimir},
  title   = {Deep Neural Nets as a Method for Quantitative Structure–Activity Relationships},
  journal = {Journal of Chemical Information and Modeling},
  year    = {2015},
  volume  = {55},
  number  = {2},
  pages   = {263-274},
  note    = {PMID: 25635324},
  doi     = {10.1021/ci500747n},
  eprint  = {https://doi.org/10.1021/ci500747n},
  url     = { 
        https://doi.org/10.1021/ci500747n
    
},
}

@Article{SchwallerTranslation,
  author      = {Philippe Schwaller and Theophile Gaudin and David Lanyi and Costas Bekas and Teodoro Laino},
  title       = {Found in Translation: Predicting Outcomes of Complex Organic Chemistry Reactions using Neural Sequence-to-Sequence Models},
  journal     = {ArXiv},
  year        = {2018},
  abstract    = {There is an intuitive analogy of an organic chemist's understanding of a compound and a language speaker's understanding of a word. Consequently, it is possible to introduce the basic concepts and analyze potential impacts of linguistic analysis to the world of organic chemistry. In this work, we cast the reaction prediction task as a translation problem by introducing a template-free sequence-to-sequence model, trained end-to-end and fully data-driven. We propose a novel way of tokenization, which is arbitrarily extensible with reaction information. With this approach, we demonstrate results superior to the state-of-the-art solution by a significant margin on the top-1 accuracy. Specifically, our approach achieves an accuracy of 80.1% without relying on auxiliary knowledge such as reaction templates. Also, 66.4% accuracy is reached on a larger and noisier dataset.},
  date        = {2017-11-13},
  eprint      = {1711.04810v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1711.04810v2:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Snapshot,
  author      = {Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
  title       = {Snapshot Ensembles: Train 1, get M for free},
  abstract    = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4% and 17.4% respectively.},
  date        = {2017-04-01},
  eprint      = {1704.00109v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1704.00109v1:PDF},
  keywords    = {cs.LG},
}

@Article{Izmailov,
  author      = {Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson},
  title       = {Averaging Weights Leads to Wider Optima and Better Generalization},
  abstract    = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  date        = {2018-03-14},
  eprint      = {1803.05407v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1803.05407v3:PDF},
  keywords    = {cs.LG, cs.AI, cs.CV, stat.ML},
}

@Article{,
  author      = {Akhilesh Gotmare and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  title       = {A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation},
  abstract    = {The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
  date        = {2018-10-29},
  eprint      = {1810.13243v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1810.13243v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{C8RA10182E,
  author    = {Karlov, Dmitry S. and Sosnin, Sergey and Tetko, Igor V. and Fedorov, Maxim V.},
  title     = {Chemical space exploration guided by deep neural networks},
  journal   = {RSC Adv.},
  year      = {2019},
  volume    = {9},
  pages     = {5151-5157},
  abstract  = {A parametric t-SNE approach based on deep feed-forward neural networks was applied to the chemical space visualization problem. It is able to retain more information than certain dimensionality reduction techniques used for this purpose (principal component analysis (PCA){,} multidimensional scaling (MDS)). The applicability of this method to some chemical space navigation tasks (activity cliffs and activity landscapes identification) is discussed. We created a simple web tool to illustrate our work (http://space.syntelly.com).},
  doi       = {10.1039/C8RA10182E},
  issue     = {9},
  publisher = {The Royal Society of Chemistry},
}

@Article{TransformerTips,
  author        = {Martin Popel and Ondřej Bojar},
  title         = {Training Tips for the Transformer Model},
  journal       = {arXiv},
  year          = {2018},
  __markedentry = {[pkarpov:1]},
  abstract      = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra "more data and larger models", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
  date          = {2018-04-01},
  doi           = {10.2478/pralin-2018-0002},
  eprint        = {1804.00247v2},
  eprintclass   = {cs.CL},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1804.00247v2:PDF},
  journaltitle  = {The Prague Bulletin of Mathematical Linguistics 110, April 2018, pp. 43-70},
  keywords      = {cs.CL},
  review        = {Important article for tricks traingin Transformer model},
}

@Article{Hinton,
  author      = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
  title       = {Distilling the Knowledge in a Neural Network},
  journal     = {arXiv},
  year        = {2015},
  abstract    = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  date        = {2015-03-09},
  eprint      = {1503.02531v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1503.02531v1:PDF},
  keywords    = {stat.ML, cs.LG, cs.NE},
}

@Article{ColeySCScore,
  author  = {Coley, Connor W. and Rogers, Luke and Green, William H. and Jensen, Klavs F.},
  title   = {SCScore: Synthetic Complexity Learned from a Reaction Corpus},
  journal = {Journal of Chemical Information and Modeling},
  year    = {2018},
  volume  = {58},
  number  = {2},
  pages   = {252-261},
  note    = {PMID: 29309147},
  doi     = {10.1021/acs.jcim.7b00622},
  url     = { 
        https://doi.org/10.1021/acs.jcim.7b00622
    
},
}

@Article{ErtlSaScore,
  author  = {Peter Ertl and Ansgar Schuffenhauer},
  title   = {Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions},
  journal = {J. Cheminform.},
  year    = {2009},
  volume  = {1},
  number  = {1},
  pages   = {8},
  issn    = {1758-2946},
  doi     = {10.1186/1758-2946-1-8},
}

@Article{ChenDrugDiscovery,
  author   = {Hongming Chen and Ola Engkvist and Yinhai Wang and Marcus Olivecrona and Thomas Blaschke},
  title    = {The rise of deep learning in drug discovery},
  journal  = {Drug Discovery Today},
  year     = {2018},
  volume   = {23},
  number   = {6},
  pages    = {1241 - 1250},
  issn     = {1359-6446},
  abstract = {Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis.},
  doi      = {https://doi.org/10.1016/j.drudis.2018.01.039},
}

@Article{ORGAN,
  author     = {Gabriel Lima Guimaraes and Benjamin Sanchez-Lengeling and Carlos Outeiral and Pedro Luis Cunha Farias and Alán Aspuru-Guzik},
  title      = {Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models},
  journal    = {arXiv},
  year       = {2017},
  abstract   = {In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.},
  date       = {2017-05-30},
  eprint     = {1705.10843v3},
  eprinttype = {arXiv},
  file       = {online:http\://arxiv.org/pdf/1705.10843v3:PDF},
  keywords   = {stat.ML, cs.LG},
}

@Article{Transformer,
  author     = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title      = {Attention Is All You Need},
  journal    = {ArXiv},
  year       = {2017},
  eprint     = {1706.03762},
  eprinttype = {arXiv},
}

@Article{SchwallerTransformer,
  author      = {Philippe Schwaller and Teodoro Laino and Théophile Gaudin and Peter Bolgar and Costas Bekas and Alpha A Lee},
  title       = {Molecular Transformer for Chemical Reaction Prediction and Uncertainty Estimation},
  journal     = {arXiv},
  year        = {2018},
  abstract    = {Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other works, we treat reaction prediction as a machine translation problem between SMILES strings of reactants-reagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable.},
  date        = {2018-11-06},
  eprint      = {1811.02633v1},
  eprintclass = {physics.chem-ph},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1811.02633v1:PDF},
  keywords    = {physics.chem-ph, cs.LG},
}

@Book{Corey,
  title     = {The Logic of Chemical Synthesis},
  publisher = {Wiley-Interscience},
  year      = {1995},
  author    = {Elias James Corey and Xue-Min Cheng},
  isbn      = {0471115940},
}

@Article{EnkvinstReview,
  author   = {Ola Engkvist and Per-Ola Norrby and Nidhal Selmi and Yu-hong Lam and Zhengwei Peng and Edward C. Sherer and Willi Amberg and Thomas Erhard and Lynette A. Smyth},
  title    = {Computational prediction of chemical reactions: current status and outlook},
  journal  = {Drug Discovery Today},
  year     = {2018},
  volume   = {23},
  number   = {6},
  pages    = {1203 - 1218},
  issn     = {1359-6446},
  abstract = {Over the past few decades, various computational methods have become increasingly important for discovering and developing novel drugs. Computational prediction of chemical reactions is a key part of an efficient drug discovery process. In this review, we discuss important parts of this field, with a focus on utilizing reaction data to build predictive models, the existing programs for synthesis prediction, and usage of quantum mechanics and molecular mechanics (QM/MM) to explore chemical reactions. We also outline potential future developments with an emphasis on pre-competitive collaboration opportunities.},
  doi      = {https://doi.org/10.1016/j.drudis.2018.02.014},
}

@Article{TetkoCNF,
  author      = {Talia B. Kimber and Sebastian Engelke and Igor V. Tetko and Eric Bruno and Guillaume Godin},
  title       = {Synergy Effect between Convolutional Neural Networks and the Multiplicity of SMILES for Improvement of Molecular Prediction},
  journal     = {arXiv},
  year        = {2018},
  abstract    = {In our study, we demonstrate the synergy effect between convolutional neural networks and the multiplicity of SMILES. The model we propose, the so-called Convolutional Neural Fingerprint (CNF) model, reaches the accuracy of traditional descriptors such as Dragon (Mauri et al. [22]), RDKit (Landrum [18]), CDK2 (Willighagen et al. [43]) and PyDescriptor (Masand and Rastija [20]). Moreover the CNF model generally performs better than highly fine-tuned traditional descriptors, especially on small data sets, which is of great interest for the chemical field where data sets are generally small due to experimental costs, the availability of molecules or accessibility to private databases. We evaluate the CNF model along with SMILES augmentation during both training and testing. To the best of our knowledge, this is the first time that such a methodology is presented. We show that using the multiplicity of SMILES during training acts as a regulariser and therefore avoids overfitting and can be seen as ensemble learning when considered for testing.},
  date        = {2018-12-11},
  eprint      = {1812.04439v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1812.04439v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{ColeyReview,
  author  = {Coley, Connor W. and Green, William H. and Jensen, Klavs F.},
  title   = {Machine Learning in Computer-Aided Synthesis Planning},
  journal = {Accounts of Chemical Research},
  year    = {2018},
  volume  = {51},
  number  = {5},
  pages   = {1281-1289},
  note    = {PMID: 29715002},
  doi     = {10.1021/acs.accounts.8b00087},
}

@Article{Pande,
  author  = {Liu, Bowen and Ramsundar, Bharath and Kawthekar, Prasad and Shi, Jade and Gomes, Joseph and Luu Nguyen, Quang and Ho, Stephen and Sloane, Jack and Wender, Paul and Pande, Vijay},
  title   = {Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models},
  journal = {ACS Central Science},
  year    = {2017},
  volume  = {3},
  number  = {10},
  pages   = {1103-1113},
  doi     = {10.1021/acscentsci.7b00303},
}

@PhdThesis{Lowe,
  author = {D. M. Lowe},
  title  = {Extraction of chemical structures and reactions from the literature},
  year   = {2012},
  url    = {https://www.repository.cam.ac.uk/handle/1810/244727},
}

@Article{Schneider,
  author  = {Schneider, Nadine and Stiefl, Nikolaus and Landrum, Gregory A.},
  title   = {What’s What: The (Nearly) Definitive Guide to Reaction Role Assignment},
  journal = {Journal of Chemical Information and Modeling},
  year    = {2016},
  volume  = {56},
  number  = {12},
  pages   = {2336-2346},
  note    = {PMID: 28024398},
  eprint  = {https://doi.org/10.1021/acs.jcim.6b00564},
}

@Article{Baylon,
  author  = {Baylon, Javier L. and Cilfone, Nicholas A. and Gulcher, Jeffrey R. and Chittenden, Thomas W.},
  title   = {Enhancing Retrosynthetic Reaction Prediction with Deep Learning Using Multiscale Reaction Classification},
  journal = {Journal of Chemical Information and Modeling},
  year    = {2019},
  volume  = {59},
  number  = {2},
  pages   = {673-688},
  note    = {PMID: 30642173},
  doi     = {10.1021/acs.jcim.8b00801},
  eprint  = {https://doi.org/10.1021/acs.jcim.8b00801},
  url     = { 
        https://doi.org/10.1021/acs.jcim.8b00801
    
},
}

@Article{Rules,
  author  = {Law, James and Zsoldos, Zsolt and Simon, Aniko and Reid, Darryl and Liu, Yang and Khew, Sing Yoong and Johnson, A. Peter and Major, Sarah and Wade, Robert A. and Ando, Howard Y.},
  title   = {Route Designer: A Retrosynthetic Analysis Tool Utilizing Automated Retrosynthetic Rule Generation},
  journal = {Journal of Chemical Information and Modeling},
  year    = {2009},
  volume  = {49},
  number  = {3},
  pages   = {593-602},
  note    = {PMID: 19434897},
  doi     = {10.1021/ci800228y},
  eprint  = {https://doi.org/10.1021/ci800228y},
  url     = { 
        https://doi.org/10.1021/ci800228y
    
},
}

@Article{BaskinReview,
  author  = {Igor I. Baskin and Timur I. Madzhidov and Igor S. Antipin and Alexandre A. Varnek},
  title   = {Artificial intelligence in synthetic chemistry: achievements and prospects},
  journal = {Russ. Chem. Rev.},
  year    = {2017},
  volume  = {86},
  number  = {11},
  pages   = {1127 - 1156},
}

@Comment{jabref-meta: databaseType:bibtex;}
